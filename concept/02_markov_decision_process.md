# Markov Decision Process (MDP)와 가치 함수

## 1. Markov Process (MP)와 Markov Reward Process (MRP) - MDP의 기본

### Markov Decision Process (MDP) 개요
- **정의**: 순차적 의사결정 과정을 수학적으로 표현한 것

### Markov Process (MP)
- **정의**: Markov 특성을 지닌, 시간에 따른 시스템의 상태 변화
- **Markov 특성**: 미래 상태는 과거 상태와 독립적이며, 현재 상태에만 의존 (Memoryless Process)
- **예시**: 아이가 잠드는 과정 (각 상태 전이 확률은 현재 상태에서 다음 상태로 넘어갈 확률)

### Markov Reward Process (MRP)
- **정의**: Markov Process의 각 상태에 보상(Reward)이 추가된 것
- **보상**: 시간에 따라 다음 상태로 넘어가면서 얻는 결과 (확률과는 독립적)

### MDP의 발전 과정
- **MP**: 상태와 상태 전이 확률만 존재
- **MRP**: MP에 보상 개념 추가
- **MDP**: MRP에 의사결정에 필요한 '행동' 개념 추가

## 2. Markov Decision Process (MDP)란?

### MDP 예시
- 그리드 월드(Grid World): 격자 형태의 환경에서 에이전트가 이동하는 문제

### MDP 구성 요소
1. **상태 (State, S)**
   - 에이전트가 관찰 가능한 상태의 집합
   - 확률 변수 (그리드 월드에서 에이전트의 위치)

2. **행동 (Action, A)**
   - 에이전트가 의사결정을 통해 취할 수 있는 행동의 집합
   - 확률 변수
   - **이산 행동 (Discrete Action)**: 정해진 행동 집합 (예: 게임의 상하좌우 이동)
   - **연속 행동 (Continuous Action)**: 연속적인 값으로 표현되는 행동 (예: 자율주행차의 속도, 조향 각도)

3. **보상 함수 (Reward Function, R)**
   - 에이전트가 특정 상태에서 특정 행동을 했을 때 얻는 보상의 기댓값
   - r(s, a) = E[R_t | S_t = s, A_t = a]

4. **상태 전이 확률 (State Transition Probability, P)**
   - 상태 s에서 행동 a를 했을 때 다음 상태 s'로 전이될 확률
   - P(s' | s, a)
   - 일반적인 게임 환경에서는 결정론적(deterministic)이지만, 현실 세계에서는 확률적(stochastic)

5. **감가율 (Discount Factor, γ)**
   - 미래 보상을 현재 시점에서 평가할 때 적용하는 할인율 (0과 1 사이의 값)
   - **사용 이유**:
     - 현재 보상과 미래 보상 구분
     - 한 번에 받는 보상과 여러 번 나눠 받는 보상 구분
     - 무한한 에피소드에서 보상 합의 발산을 방지하고 수렴하게 함

## 3. Return 값이란?

### 정의와 수식
- **정의**: 현재 시점부터 에피소드가 끝날 때까지 얻을 수 있는 (감가된) 보상의 총합
- **수식**: G_t = R_(t+1) + γR_(t+2) + γ^2R_(t+3) + ...

### 의미
- 감가율을 통해 가까운 미래의 보상을 더 크게, 먼 미래의 보상을 더 작게 평가
- 미래에 대한 불확실성을 감가율을 통해 표현

### Return 값의 한계
- 에피소드가 끝나기 전에는 정확한 값을 알 수 없음 (추정은 가능)

## 4. 가치 함수와 큐 함수

### 가치 함수 (Value Function, v(s))
- **정의**: 현재 상태 s에서 기대되는 Return 값
- **의미**: 현재 상태의 가치를 판별하는 기준
- **목표**: 강화학습의 목표는 가치 함수를 최대화하는 것
- **수식**:
  - v(s) = E[G_t | S_t = s]
  - v(s) = E[R_t + γv(S_(t+1)) | S_t = s] (벨만 기대 방정식)

### 큐 함수 (Q-Function, q(s, a))
- **정의**: 현재 상태 s에서 행동 a를 취했을 때 기대되는 Return 값 (Action-Value Function)
- **의미**: 현재 상태에서 특정 행동의 가치를 평가하는 기준
- **수식**:
  - q(s, a) = E[G_t | S_t = s, A_t = a]
  - q(s, a) = E[R_t + γq(S_(t+1), A_(t+1)) | S_t = s, A_t = a] (벨만 기대 방정식)

### 가치 함수와 큐 함수의 비교
- **차이점**:
  - **가치 함수**: 현재 *상태*의 가치 평가
  - **큐 함수**: 현재 *상태*에서 특정 *행동*의 가치 평가 (행동(a)이 변수로 추가됨)

### 관계
- 가치 함수는 특정 상태에서 선택 가능한 모든 행동에 대한 큐 함수 값의 (행동 확률에 따른) 가중 평균
- **수식**: vπ(s) = ∑ π(a|s)qπ(s, a)
  - π(a|s): 정책(policy). 상태 s에서 행동 a를 선택할 확률
  - ∑ π(a|s) = 1 (모든 행동 확률의 합은 1)

### 정책 (Policy, π)
- 어떤 상태에서 각 행동을 할 확률
- 에이전트는 학습을 통해 Return 값을 최대화하는 최적 정책으로 업데이트

## 5. 벨만 방정식 (Bellman Equation)

### 벨만 기대 방정식 (Bellman Expectation Equation)
- **목적**: 가치 함수를 계산 가능하게 만들기 위한 방정식
- **특징**: 가치 함수/큐 함수의 기댓값 연산을 효율적으로 수행할 수 있도록 식을 풀어 쓴 형태
- **의미**: 상태가 많은 환경에서 여러 번의 연속적인 계산으로 가치 함수/큐 함수 값을 구할 수 있게 함
- **기본 형태**:
  - v(s) = E[R_t + γv(S_(t+1)) | S_t = s] (가치 함수)
  - q(s, a) = E[R_t + γq(S_(t+1), A_(t+1)) | S_t = s, A_t = a] (큐 함수)

### 계산 가능한 벨만 기대 방정식
- **벨만 기대 방정식 변형**:
  - vπ(s) = Eπ[R_t + γvπ(S_(t+1)) | S_t = s]
  - 기댓값의 정의를 이용하여 전개: E[X] = ∑ p_i * x_i
    - p_i: x_i 사건이 일어날 확률
    - x_i: 발생 가능한 사건
  - 최종적으로, 현재 상태의 가치 함수를 다음 상태의 가치 함수와 보상의 조합으로 표현 가능

### 벨만 최적 방정식 (Bellman Optimality Equation)

#### 최적 가치 함수 (Optimal Value Function, v*(s))
- 현재 상태에서 최적 정책을 따랐을 때의 가치 함수
- 모든 가치 함수 중 가장 큰 값
- v*(s) = maxπ [vπ(s)]

#### 최적 큐 함수 (Optimal Q-Function, q*(s, a))
- 현재 상태 s에서 행동 a를 하고, 그 이후에 최적 정책을 따를 때의 가치함수
- q*(s,a) = maxπ [qπ(s,a)]

#### 최적 정책 (Optimal Policy, π*(s, a))
- 가치 함수를 최대로 하는 정책
- π*(s, a) = 1 (if a = argmax_(a∈A) q*(s, a)), 0 (otherwise)

#### 벨만 최적 방정식 유도
- **가치 함수**:
  1. v*(s) = maxπ [vπ(s)]
  2. π*(s, a) = 1 (if a = argmax_(a∈A) q*(s, a)), 0 (otherwise)
  3. vπ(s) = ∑ π(a|s)qπ(s, a) -> vπ(s) = qπ(s, a) (최적 정책 하에서는 하나의 행동만 선택)
  4. v*(s) = max_a q*(s, a)
  5. qπ(s, a) = Eπ[R_t + γqπ(S_(t+1), A_(t+1)) | S_t = s, A_t = a]
  6. v*(s) = max_a Eπ[R_t + γqπ(S_(t+1), A_(t+1)) | S_t = s, A_t = a]
  7. v*(s) = max_a Eπ[R_t + γvπ(S_(t+1)) | S_t = s, A_t = a]

- **큐 함수**:
  1. q*(s, a) = maxπ [qπ(s, a)]
  2. qπ(s, a) = Eπ[R_t + γqπ(S_(t+1), A_(t+1)) | S_t = s, A_t = a]
  3. q*(s, a) = E[R_t + γ max_a q*(S_(t+1), A_(t+1)) | S_t = s, A_t = a]

## 6. Dynamic Programming (DP)

### 정의와 특징
- **정의**: 순차적 행동 문제를 푸는 방식
- **특징**:
  - 전체 문제를 작은 문제로 나누어 해결
  - 시간 순서대로 연쇄적으로 문제를 해결
  - 공통으로 해당하는 문제를 해결하여 계산량을 줄임 (재귀 함수와 유사하지만, 계산 값을 저장)

### 벨만 방정식과 DP
- 벨만 방정식은 DP를 사용하여 가치 함수/큐 함수를 반복적으로 계산하는 데 활용됨
- 예: v(s5) → v(s4) → v(s3) → ... 순으로 계산

### DP를 활용한 강화학습 알고리즘
- **정책 평가 (Policy Evaluation)**: 주어진 정책의 가치 함수를 계산
- **정책 개선 (Policy Improvement)**: 현재 정책보다 더 좋은 정책을 찾음
- **정책 반복 (Policy Iteration)**: 정책 평가와 정책 개선을 반복
- **가치 반복 (Value Iteration)**: 최적 가치 함수를 직접 계산하여 최적 정책을 도출

## 요약

1. **MP와 MRP**: MDP는 Markov Process와 Markov Reward Process를 기반으로 발전
2. **MDP 구성 요소**: 상태, 행동, 보상 함수, 상태 전이 확률, 감가율의 5가지 요소로 이루어짐
3. **Return 값**: 현재부터 미래까지 얻을 수 있는 감가된 보상의 총합
4. **가치 함수와 큐 함수**:
   - **가치 함수 (v(s))**: 현재 상태의 가치
   - **큐 함수 (q(s, a))**: 현재 상태에서 특정 행동의 가치
5. **벨만 기대 방정식**: 가치 함수/큐 함수 계산을 효율적으로 만드는 방정식
6. **벨만 최적 방정식**: 최적 가치 함수/큐 함수를 구하기 위한 방정식
7. **Dynamic Programming**: 큰 문제를 작은 문제로 나누어 순차적으로 해결, 계산 효율성 높임
