# 탐험(Exploration)과 활용(Exploitation) 및 강화학습 알고리즘

## 1. 탐험과 활용의 개념

### 최적 정책 (Optimal Policy)
- **정의**: 최적 가치 함수를 도출할 수 있는 정책
- **수식**: π*(s, a) = 1 (if a = argmax_(a∈A) q*(s, a)), 0 (otherwise)
- **의미**: 주어진 상태에서 큐 함수를 최대로 만드는 행동을 선택하는 정책

### 탐욕 정책 (Greedy Policy)
- **정의**: (현재 시점에서) 최대 보상만을 쫓는 정책
- **문제점**: 편향된 결과, 잘못된 학습 방향으로 이어질 수 있음
- **한계**: 경험이 부족하면 큐 함수 추정이 부정확할 수 있음

### 탐험 (Exploration)
- **정의**: 랜덤한 선택을 통해 탐욕 정책의 단점을 보완하는 행동 전략
- **목적**: 낮은 확률로 편향된 결과에서 벗어나 더 나은 결과를 탐색
- **예시**: 그리드 월드에서 무작위 이동을 통해 새로운 경로 발견

### 활용 (Exploitation)
- **정의**: (현재까지 학습된) 높은 보상을 추구하는 탐욕 정책을 따르는 행동 전략
- **목적**: 현재까지 얻은 지식을 바탕으로 최대한의 보상을 획득
- **예시**: 그리드 월드에서 (현재까지 학습된) 최적 경로 선택
- **주의점**: 탐험 없이 활용만 할 경우, 국소 최적해(local optimum)에 빠질 수 있음

### ε-greedy 정책
- **정의**: 탐험과 활용의 균형을 맞추는 방법
- **작동 방식**:
  - ε (epsilon) 확률로 무작위 행동 선택 (탐험)
  - 1-ε 확률로 최대 큐 함수를 갖는 행동 선택 (활용)
- **수식**:
  ```
  π(a|s) = { ε/|A(s)| + 1 - ε  if a* = argmax_a Q(s, a)
           { ε/|A(s)|          otherwise
  ```
  - |A(s)|: 상태 s에서 가능한 행동의 개수
  - a*: 큐 함수를 최대화하는 행동

## 2. Monte Carlo 예측법 (Monte Carlo Prediction)

### 정의와 원리
- **정의**: 샘플링 평균을 통해 가치 함수와 정책을 업데이트하는 방식
- **원리**: 에피소드가 끝난 후 얻는 Return 값(G_t)을 이용하여 가치 함수 추정
- **예시**: 원의 넓이 구하기
  - 전체 영역(B) 안에 무작위로 점을 찍음
  - 원(A) 안에 들어간 점의 비율을 통해 원의 넓이 근사치 계산
  - 점이 많을수록 근사치는 실제 값에 수렴

### Return 값을 이용한 가치 함수 추정
- **Return 값 (G_t)**: G_t = R_t + γR_(t+1) + ... + γ^(T-t+1)R_T
- **가치 함수 추정**: 마지막 에피소드까지 진행한 후 누적된 Return 값의 평균
- **업데이트 수식**: V_n = V_(n-1) + α(G_n - V_(n-1))
  - α: 학습률 (Learning Rate)
  - G_n: n번째 에피소드의 Return 값
  - V_(n-1): 이전 가치 함수 추정값

### Monte Carlo 예측의 특징

#### 장점
- 에피소드가 충분하고 길이가 짧으면 근사치에 매우 가까운 예측값 제공
- 낮은 편향(Bias): 충분한 샘플이 있을 때 실제 값에 수렴

#### 단점
- 에피소드의 길이가 너무 길거나 양이 적으면 부적합 (높은 분산)
- 정책 업데이트가 실시간으로 되지 않음 (매 에피소드 종료 시 업데이트)
- 무한한 길이의 에피소드나 너무 긴 에피소드에 적용하기 어려움

#### 편향(Bias)과 분산(Variance)
- **편향**: 예측값이 목표값이 아닌 다른 값으로 수렴하는 경향
- **분산**: 예측값이 목표값 주변에 얼마나 넓게 퍼져있는지의 정도
- **Monte Carlo 특성**: 낮은 편향, 높은 분산
  - 샘플 수가 무한히 커지면 예측값이 실제 값에 거의 수렴

## 3. TD 알고리즘 (Temporal Difference Algorithm)

### 정의와 원리
- **정의**: 시간차 예측을 통해 가치 함수를 업데이트하는 알고리즘
- **원리**: 매 타임스텝마다 예측 및 업데이트 진행
- **부트스트랩 (Bootstrap) 방식**: 이전 정책으로 예측한 가치 함수를 토대로 다음 스텝의 가치 함수 예측

### TD 알고리즘의 업데이트 방식
- **업데이트 수식**: V(S_t) ← V(S_t) + α(R_t + γV(S_(t+1)) - V(S_t))
  - α: 학습률 (Learning Rate)
  - R_t + γV(S_(t+1)): 업데이트 목표 (TD Target)
  - R_t + γV(S_(t+1)) - V(S_t): 시간차 에러 (TD Error)

### SARSA 알고리즘 - TD 기반의 On-policy 알고리즘
- **정의**: 상태-행동-보상-다음상태-다음행동 정보를 활용하는 알고리즘
- **업데이트 요소**: [S_t, A_t, R_t, S_(t+1), A_(t+1)]
- **큐 함수 업데이트 수식**: Q(S_t, A_t) ← Q(S_t, A_t) + α(R_t + γQ(S_(t+1), A_(t+1)) - Q(S_t, A_t))
- **특징**:
  - 다음 상태의 행동까지 고려하여 큐 함수 업데이트
  - On-policy: Behavior Policy의 탐험이 Target Policy 업데이트에 영향
  - 탐험 중 좋지 않은 결과도 학습에 반영됨

### TD 알고리즘의 특징

#### 장점
- 실시간 업데이트 가능 (매 타임스텝마다 업데이트)
- 무한하거나 긴 에피소드에도 적용 가능
- Monte Carlo 예측보다 빠른 학습 속도

#### 단점
- 초기 가치 함수 값에 따라 예측 정확도가 크게 달라질 수 있음
- 부트스트랩 방식으로 인한 편향 가능성
- On-policy 특성상 탐험 중 좋지 않은 결과가 학습에 영향을 줄 수 있음

## 4. Monte Carlo 예측과 TD 알고리즘 비교

| 특징 | Monte Carlo 예측 | TD 알고리즘 |
| --- | --- | --- |
| 목표 함수 | G_t (실제 Return 값) | R_t + γV(S_(t+1)) (다음 상태 가치 함수를 이용한 예측값) |
| 업데이트 시점 | 에피소드 종료 후 | 매 타임스텝 |
| 학습 방식 | 실제 경험 기반 | 부트스트랩 기반 |
| 편향-분산 | 낮은 편향, 높은 분산 | 높은 편향, 낮은 분산 |
| 장점 | 에피소드가 충분하고 짧으면 정확한 예측 | 실시간 업데이트, 무한/긴 에피소드에 적합 |
| 단점 | 실시간 업데이트 불가, 긴 에피소드에 부적합, 높은 분산 | 초기 가치 함수에 민감, 편향 가능성 |
| 적합한 상황 | 짧은 에피소드, 정확한 최종 가치 필요 | 긴 에피소드, 실시간 학습 필요 |

## 요약

1. **탐험과 활용**:
   - 탐험: 랜덤 선택을 통해 새로운 가능성 탐색
   - 활용: 현재까지 학습된 정보를 바탕으로 최적 행동 선택
   - ε-greedy 정책: 탐험과 활용의 균형 유지 방법

2. **최적 정책**:
   - 학습을 통해 가치 함수/큐 함수를 최대화하는 정책
   - 모든 상태에서 최적의 행동을 선택하는 정책

3. **Monte Carlo 예측**:
   - 에피소드 종료 후 실제 Return 값을 바탕으로 가치 함수 업데이트
   - 낮은 편향, 높은 분산 특성
   - 짧은 에피소드에 적합

4. **TD 알고리즘**:
   - 매 타임스텝마다 시간차 예측으로 가치 함수 업데이트
   - 높은 편향, 낮은 분산 특성
   - 긴 에피소드에 적합, 실시간 업데이트 가능
   - SARSA는 On-policy TD 알고리즘의 예시
